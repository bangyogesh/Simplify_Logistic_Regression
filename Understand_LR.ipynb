{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "div.cell { /* Tunes the space between cells */\n",
       "margin-top:1em;\n",
       "margin-bottom:1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
       "font-size: 2.0em;\n",
       "line-height:1.6em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
       "margin-bottom: 1em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.2em;\n",
       "line-height:1.2em;\n",
       "padding-left:1em;\n",
       "padding-right:3em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.0em;\n",
    "line-height:1.6em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: 1em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.2em;\n",
    "line-height:1.2em;\n",
    "padding-left:1em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simplify - Logistic Regression\n",
    "\n",
    "## It's all about binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_0.gif\" \n",
    "align=\"middle\" alt=\"Image_1_0\" data-canonical-src=\"\" style=\"width:100%;height:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## There are main three sections of this workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "                Section 1:  Understanding - Logistic Regression\n",
    "\n",
    "                Section 2:  Building - Logistic Regression from scratch\n",
    "\n",
    "                Section 3:  Classify emails into Spam and Ham \n",
    "                \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "        1.1 Intorduction to Machine Learning\n",
    "        \n",
    "        1.2 Intuition behind the Logistic Regression (LR)\n",
    "        \n",
    "        1.4 Math behind the LR\n",
    "        \n",
    "        1.5 Summary\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_1.jpeg\" \n",
    "align=\"middle\" alt=\"Image_1_1\" data-canonical-src=\"\" style=\"width:80%;height:80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_2.png\" \n",
    "align=\"middle\" alt=\"Image_1_2\" data-canonical-src=\"\" style=\"width:60%;height:60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intuition behind the Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_3.jpeg\" \n",
    "align=\"middle\" alt=\"Image_1_3\" data-canonical-src=\"\" style=\"width:70%;height:70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;font-size:11px;\">Image Source is [here](https://cdn-images-1.medium.com/max/1600/1*D70wG-VO7GFq85VFejundA.jpeg)</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center;\">Logistic Regression is as simple as dividing waste!</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression is basically Supervised ML algorithm that is used for __binary classification__\n",
    "* Our main goal is to __predict the probability of the discrete class labels__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!! Warning !!!\n",
    "\n",
    "* __Don't get confuse__ with the name __\"Linear Regression\"__ and __\"Logistic Regression\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math behind the LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why can't we apply Linear Regression for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_4.png\" \n",
    "align=\"middle\" alt=\"Image_1_4\" data-canonical-src=\"\" style=\"width:70%;height:70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_5.png\" \n",
    "align=\"middle\" alt=\"Image_1_5\" data-canonical-src=\"\" style=\"width:70%;height:70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For __classification task__ we know that our output is __either a positive class(1) or negative class(0)__\n",
    "\n",
    "\n",
    "* Now if we are using __Linear Regression then it's hypothesis function can generate much larger value than 1 for positive class whereas our training example labeled as 1.__\n",
    "\n",
    "\n",
    "* Similarly, __Linear Regression's hypothesis function can generate much smaller value than 0 for negative class whereas our training example labeled as 0.__\n",
    "\n",
    "\n",
    "* Mathematically, our Linear Regression __hypothesis function $h_{\\theta}(x)$ can be >1 or <0__\n",
    "\n",
    "\n",
    "* We need __hypothesis function which can be satisfy this math rule:__ $0 \\leq h_{\\theta}(x)\\leq 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose Sigmoid function / Logistic function as our hypothesis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_6.gif\" \n",
    "align=\"middle\" alt=\"Image_1_6\" data-canonical-src=\"\" style=\"width:70%;height:70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* This function will give us the __output value between [0,1]__\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "* When you __perform derivative of $e^{x}$ it gives you back $e^{x}$ and this function is the only known \n",
    "  differentiable function with this mathematical property. That makes it more convenient for calculation.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Assumption of Logistic Regression:__ \n",
    "\n",
    "* Input space can be __separated into two nice ‘regions’, one for each class, by a linear(read: straight) boundary.__\n",
    "\n",
    "\n",
    "\n",
    "* Your data must be linearly separable in __n dimensions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_7.png\" \n",
    "align=\"middle\" alt=\"Image_1_7\" data-canonical-src=\"\" style=\"width:40%;height:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive Example to understand math behind LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example / Problem statement: __\n",
    "\n",
    "* We are provided a sample of 100 customers. \n",
    "\n",
    "\n",
    "* We need to predict the probability whether a customer will buy (y) a particular magazine or not based on their \n",
    "  age\n",
    "\n",
    "\n",
    "* As you can see, we’ve a categorical outcome variable, we’ll use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with logistic regression, I’ll first write the simple linear regression equation with dependent variable\n",
    "\n",
    "$$ \\large Y = mx + C ------ (1) $$\n",
    "\n",
    "Here,\n",
    "   * Y = dependent variable ( user buy magazine or not)\n",
    "   * m = slop\n",
    "   * x  = independent variable ( Here I will take age of the readers for make it simple)\n",
    "   * C = Y-intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, final equation form based on our example will be\n",
    "\n",
    "$$ \\large Y_{readers\\;will\\;buy\\; magazine\\; or\\; not}  = m * x_{age} + C ------(2)$$\n",
    "\n",
    "\n",
    "* Now consider Y as function of probability. Why? because we need the chance (probability of the whether buyer will buy the magazine or not)\n",
    "\n",
    "\n",
    "* This function is established using two things: \n",
    "  * Probability of Success(p) means reader buys magazine \n",
    "  * Probability of Failure(1-p) means reader doesn't buy magazine.\n",
    "  \n",
    "* Probability p should meet following criteria:\n",
    "    1. It must always be positive (since p >= 0)\n",
    "    2. It must always be less than equals to 1 (since p <= 1)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Since probability must always be positive, we’ll put the linear equation in exponential form.__\n",
    "For any value of slope and dependent variable, exponent of this equation will never be negative.\n",
    "\n",
    "\n",
    "$$ \\large p = exp(C + mx_{Age}) = e^{(C + mx_{Age})}    ------- (3) $$ \n",
    "\n",
    "Here,\n",
    "\n",
    "* P  =  Probability of readers' buy magazine or not\n",
    "* exp OR e =  exponential symbol OR Euler's number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the probability less than 1, we must divide p by a number greater than p. This can simply be done by:\n",
    "\n",
    "$$ \\large p  =  \\frac{exp(C + mx_{Age})}{exp(C + mx_{Age}) + 1}   =  \\frac{ e^{(C + mx_{Age})}}{e^{(C + mx_{Age})} + 1 }   ----- (4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by using equation 1, 2, 3 and 4 we can redefine the probability as:\n",
    "\n",
    " $$ p = \\frac {e^{Y}}{ 1 + e^{Y}} ----- (5)$$ \n",
    " \n",
    " * Here we have just __substitute the C + mx{Age} with Y__\n",
    " \n",
    " \n",
    " * Here p is the probability of success. This __Equation (5) is the Logit Function__\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "If p is the probability of success, 1-p will be the probability of failure which can be written as:\n",
    "\n",
    "$$q = 1 - p = 1 - \\frac {e^{Y}}{ 1 + e^{Y}}    --- (6)$$\n",
    "\n",
    "* Here q is the probability of failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's __divide the equation 5 and 6__ so that we get following equation\n",
    "\n",
    "\n",
    "$$ \\frac {p}{1-p} = e^{Y}    --- (7)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After __taking log on both side__, So that we can get the value of Y\n",
    "\n",
    "$$ \\large log(\\frac{p}{1-p}) = Y    --- (8) $$\n",
    "\n",
    "Now, let's __put the value of Y in equation 8 from equation 2__\n",
    "\n",
    "$$ \\large log(\\frac{p}{1-p}) = C + mx_{Age}   --- (9) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "\n",
    "* The __equation 9 is used in Logistic Regression and It is called Logit(p). __\n",
    "\n",
    "\n",
    "* Here __(p/1-p) is called the Odd Ratio (OR) or Odds(p)__\n",
    "\n",
    "\n",
    "* Whenever the __log of Odd Ratio is found to be positive, then the probability of success is always more than \n",
    "  50%.__\n",
    "\n",
    "\n",
    "* You can see __probability never goes below 0 and above 1. __\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we take odds instead of probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let $P(X)$ denote the probability of an event X occurring. In that case, the __odds ratio (OR(X))__ is defined as\n",
    "\n",
    "$$ \\large \\frac{P(X)}{1-P(X)}$$\n",
    "\n",
    "* It is essentially the ratio of the probability of the event happening, vs. it not happening.\n",
    "\n",
    "\n",
    "* It is clear that __probability and odds__ convey the __exact same information.__ \n",
    "\n",
    "\n",
    "* __But as $P(X)$ goes from 0 to 1, OR(X) goes from 0 to infinity__\n",
    "\n",
    "\n",
    "* The advantage is that the __odds defined on 0 to infinity__ which map to __log-odds on -infinity to +infinity__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formally Introduce: Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can't we directly use cost function of linear regression which is Sum of sequared Errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Answer is __No, we can't__ \n",
    "\n",
    "\n",
    "* Linear regression uses mean squared error as its cost function. If this is used for logistic regression, then it \n",
    "  will be generate a non-convex function of our input intercept values or parameter (theta) values\n",
    "  \n",
    "    \n",
    "* Gradient Descent will converge into global minimum for function that is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_8.png\" \n",
    "align=\"middle\" alt=\"Image_1_8\" data-canonical-src=\"\" style=\"width:80%;height:80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training Set is define as:__ $ \\large \\{ {(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})} \\}$\n",
    "\n",
    "* There are __m training examples__\n",
    "\n",
    "__Here x is referred as:__ $  x \\in  \\begin{bmatrix}\n",
    "  x_{0} = 1 \\\\\n",
    "  x_{1} \\\\\n",
    "  x_{2} \\\\\n",
    "  \\vdots \\\\\n",
    "  x_{m} \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "__Here y is referred as:__ $ y \\in \\{ 0, 1\\} $\n",
    "\n",
    "\n",
    "\n",
    "__Hypothesis function is the sigmoid function: __ $ \\Large h_{\\theta}(x) = \\frac{1}{1\\;+\\; e^{-z}} = \\frac{1}{1\\;+\\; e^{-\\theta^{T}}{x}}$\n",
    "\n",
    "\n",
    "__Our goal is to find the values of parameter $\\theta$ and in our last example it is the value of $ m \\; and \\; c $__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will take log-likelihood estimation of sigmoid function for defining our cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function is define as below\n",
    "\n",
    "$$ \\large \\text{Cost function} (h_{\\theta}(x))  = \\begin{cases}\n",
    "  -\\log(h_{\\theta}(x)) & \\text{if $y = 1$} \\\\\n",
    "  -\\log(1\\; -\\; h_{\\theta}(x) ) & \\text{if $y = 0$}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_9.png\" \n",
    "align=\"middle\" alt=\"Image_1_9\" data-canonical-src=\"\" style=\"width:80%;height:80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Cost function for entire data set with m observation} = J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\text{Cost function} (h_{\\theta}(x^{i},y^{i})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine both cases of Cost function in one equation\n",
    "\n",
    "$$ \\large \\text{Cost function} (h_{\\theta}(x),y)  = \n",
    "  -y\\log(h_{\\theta}(x))-(1-y)\\log(1\\; -\\; h_{\\theta}(x) )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large J(\\theta) = - \\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_{\\theta}(x^{(i)}) + (1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we need to minimize this cost function $J(\\theta)$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to minimize the cost function we need to derive its __partial derivative with respect to $\\theta$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large J(\\theta) = - \\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(\\frac{1}{1+e^{-\\theta x^{(i)}}}) + (1-y^{(i)})\\log(1-\\frac{1}{1+e^{-\\theta x^{(i)}}})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Partial derivate that we use for updating our cost function or minimize our error function is given below__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\frac{\\partial J}{\\partial \\theta}  = \\frac{1}{m}\\sum_{i=1}^{m}[x^{(i)}_{j}h_{\\theta}(x^{(i)}) - y^{(i)}x^{(i)}_{j}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Gradient descent equation is as below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Repeat this until we get $min_{\\theta}J(\\theta)$:__\n",
    "\n",
    "        {\n",
    "\n",
    "$$ \\large \\theta_{j} := \\theta_{j} -\\alpha \\sum_{i=0}^{m}[x^{(i)}_{j}h_{\\theta}(x^{(i)}) - y^{(i)}x^{(i)}_{j}] $$\n",
    "\n",
    "        }\n",
    "        \n",
    "\n",
    "__Here $\\alpha$ is Learning Rate __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Classification & Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Simplify_Logistic_Regression/master/imags/Image_1_10.png\" \n",
    "align=\"middle\" alt=\"Image_1_10\" data-canonical-src=\"\" style=\"width:60%;height:60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Properties| Supervised Classification (Logistic Regression) | Regression (Linear Regression)|\n",
    "|:---:|:---: |:---:|\n",
    "|  Output Type | Discrete Value (Class labels) | Continuous (number) |\n",
    "|  What are you trying to achieve? | Decision Boundary or How to choose value for parameters theta | Best line of fit |\n",
    "|  Evaluation | Accuracy | Sum of Sequared Error or $r^{2} Error$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to remember what we have learnt so far....!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some reading resource\n",
    "\n",
    "* [Refer Coursera Videos 6.1 to 6.7](https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)\n",
    "\n",
    "\n",
    "* [Maximum Liklihood Explanation](https://pmirla.github.io/2016/07/20/maximum-likelihood-explanation.html)\n",
    "\n",
    "\n",
    "* [Derivative of cost function](https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression)\n",
    "\n",
    "\n",
    "* [Solve partial derivative step by step](https://www.symbolab.com)\n",
    "\n",
    "\n",
    "* [Why LR cost function has logarithmic expression](https://stackoverflow.com/questions/32986123/why-the-cost-function-of-logistic-regression-has-a-logarithmic-expression)\n",
    "\n",
    "\n",
    "* [Deriving cost function for LR](https://math.stackexchange.com/questions/886555/deriving-cost-function-using-mle-why-use-log-function)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
